{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# # This Python 3 environment comes with many helpful analytics libraries installed\n","# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# # For example, here's several helpful packages to load\n","\n","# import numpy as np # linear algebra\n","# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# # Input data files are available in the read-only \"../input/\" directory\n","# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","# import os\n","# for dirname, _, filenames in os.walk('/kaggle/input'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))\n","\n","# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-08-17T17:03:57.541736Z","iopub.status.busy":"2023-08-17T17:03:57.541365Z","iopub.status.idle":"2023-08-17T17:04:00.723071Z","shell.execute_reply":"2023-08-17T17:04:00.722171Z","shell.execute_reply.started":"2023-08-17T17:03:57.541702Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(11999, 476)"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import numpy as np\n","from surprise import Dataset, Reader, SVD\n","from surprise.model_selection import train_test_split\n","from surprise import accuracy\n","\n","# Load data\n","data = pd.read_csv('/kaggle/input/amazon-ratings/ratings_Beauty.csv')\n","data.drop(['Timestamp'], axis=1)\n","data = data.iloc[2500:15000]\n","\n","# Map user and product IDs to numerical indices\n","user_mapping = {user_id: idx for idx, user_id in enumerate(data['UserId'].unique())}\n","product_mapping = {product_id: idx for idx, product_id in enumerate(data['ProductId'].unique())}\n","\n","# Create numerical user and product IDs in the DataFrame\n","data['UserIndex'] = data['UserId'].map(user_mapping)\n","data['ProductIndex'] = data['ProductId'].map(product_mapping)\n","\n","# Create user-item interaction matrix\n","\n","# data = data.head(10000)\n","testing = data\n","interaction_matrix = pd.pivot_table(data, values='Rating', index='UserIndex', columns='ProductIndex', fill_value=0)\n","interaction_matrix.shape"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-08-17T17:10:59.654408Z","iopub.status.busy":"2023-08-17T17:10:59.654036Z","iopub.status.idle":"2023-08-17T17:10:59.663093Z","shell.execute_reply":"2023-08-17T17:10:59.662259Z","shell.execute_reply.started":"2023-08-17T17:10:59.654377Z"},"trusted":true},"outputs":[],"source":["import pickle\n","\n","# Save user_mapping to a file\n","with open('user_mapping.pkl', 'wb') as file:\n","    pickle.dump(user_mapping, file)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-08-17T17:11:39.002692Z","iopub.status.busy":"2023-08-17T17:11:39.002021Z","iopub.status.idle":"2023-08-17T17:11:39.007993Z","shell.execute_reply":"2023-08-17T17:11:39.006929Z","shell.execute_reply.started":"2023-08-17T17:11:39.002652Z"},"trusted":true},"outputs":[],"source":["import pickle\n","\n","with open('product_mapping.pkl', 'wb') as file:\n","    pickle.dump(product_mapping, file)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-08-17T17:04:04.333286Z","iopub.status.busy":"2023-08-17T17:04:04.332930Z","iopub.status.idle":"2023-08-17T17:04:34.409183Z","shell.execute_reply":"2023-08-17T17:04:34.408187Z","shell.execute_reply.started":"2023-08-17T17:04:04.333254Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","\n","from surprise import SVD\n","from surprise import dataset\n","from surprise import Reader\n","\n","# Apply SVD to get initial embeddings\n","U, sigma, Vt = np.linalg.svd(interaction_matrix)\n","\n","# Number of latent features\n","latent_features = 10\n","\n","# Truncate U, sigma, and Vt matrices to the desired number of latent features\n","U = U[:, :latent_features]\n","sigma = np.diag(sigma[:latent_features])\n","Vt = Vt[:latent_features, :]\n","\n","# Initialize user and product embeddings from U and Vt\n","user_embeddings = U\n","product_embeddings = Vt.T\n","\n","# Hyperparameters\n","learning_rate = 0.01\n","reg_lambda = 0.01  # Regularization parameter\n","num_epochs = 10"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-08-17T17:04:34.411610Z","iopub.status.busy":"2023-08-17T17:04:34.411188Z","iopub.status.idle":"2023-08-17T17:04:34.448605Z","shell.execute_reply":"2023-08-17T17:04:34.447557Z","shell.execute_reply.started":"2023-08-17T17:04:34.411574Z"},"trusted":true},"outputs":[],"source":["#Â You'll need to create a dummy reader\n","reader = Reader(line_format='user item rating', rating_scale=(1, 5))\n","\n","# # # Also, a dummy Dataset class\n","class MyDataset(dataset.DatasetAutoFolds):\n","\n","    def __init__(self, df, reader):\n","\n","        self.raw_ratings = [(uid, iid, r, None) for (uid, iid, r) in\n","                            zip(df['UserIndex'], df['ProductIndex'], df['Rating'])]\n","        self.reader=reader\n","\n","data = MyDataset(data, reader)\n","\n","train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # Training loop\n","for epoch in range(51):\n","    total_loss = 0\n","\n","    for user_idx, item_idx, rating in train_data.all_ratings():\n","        predicted_rating = np.dot(user_embeddings[user_idx], product_embeddings[item_idx])\n","        error = rating - predicted_rating\n","        \n","        # Update embeddings using gradient descent\n","        user_embeddings[user_idx] += learning_rate * (error * product_embeddings[item_idx] - reg_lambda * user_embeddings[user_idx])\n","        product_embeddings[item_idx] += learning_rate * (error * user_embeddings[user_idx] - reg_lambda * product_embeddings[item_idx])\n","        \n","        total_loss += error ** 2\n","\n","    # Calculate and print average loss for the epoch\n","    average_loss = total_loss / train_data.n_ratings"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-08-17T17:05:27.488969Z","iopub.status.busy":"2023-08-17T17:05:27.488622Z","iopub.status.idle":"2023-08-17T17:05:27.501368Z","shell.execute_reply":"2023-08-17T17:05:27.500383Z","shell.execute_reply.started":"2023-08-17T17:05:27.488940Z"},"trusted":true},"outputs":[],"source":["np.save('user_embeddings.npy', user_embeddings)\n","np.save('product_embeddings.npy', product_embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_predictions = []\n","for user_idx, item_idx, _ in test_data:\n","    predicted_rating = np.dot(user_embeddings[user_idx], product_embeddings[item_idx])\n","    test_predictions.append(predicted_rating)\n","\n","# Convert test_predictions to a numpy array\n","test_predictions = np.array(test_predictions)\n","\n","# Calculate and print RMSE and MAE\n","actual_ratings = np.array([rating for _, _, rating in test_data])\n","rmse = np.sqrt(np.mean((test_predictions - actual_ratings) ** 2))\n","mae = np.mean(np.abs(test_predictions - actual_ratings))"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-08-17T17:04:53.469001Z","iopub.status.busy":"2023-08-17T17:04:53.468637Z","iopub.status.idle":"2023-08-17T17:04:53.474404Z","shell.execute_reply":"2023-08-17T17:04:53.473457Z","shell.execute_reply.started":"2023-08-17T17:04:53.468971Z"},"trusted":true},"outputs":[{"data":{"text/plain":["__main__.MyDataset"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["type(data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Test recommendations\n","user_index_to_test = user_mapping['AT7EYU8AKND5V']\n","test_products = list(testing[testing['UserIndex'] == user_index_to_test]['ProductIndex'])\n","\n","recommendations = []\n","for product_idx in range(len(product_mapping)):\n","    if product_idx not in test_products:\n","        if user_index_to_test < len(user_embeddings) and product_idx < len(product_embeddings):\n","            predicted_rating = np.dot(user_embeddings[user_index_to_test], product_embeddings[product_idx])\n","            recommendations.append((product_idx, predicted_rating))\n","\n","# Rank products based on predicted ratings\n","recommendations = sorted(recommendations, key=lambda x: x[1], reverse=True)\n","\n","# Print top N recommendations\n","top_recommendations = recommendations[:10]\n","for product_idx, predicted_rating in top_recommendations:\n","    product_id = next(key for key, value in product_mapping.items() if value == product_idx)\n","    print(f\"Recommended Product: {product_id}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
